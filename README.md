Follows https://arxiv.org/pdf/1706.03762 somewhat, but only the decoder part as GPT is a decoder only model

- Video to follow along all the steps and questions you have https://www.youtube.com/watch?v=kCc8FmEb1nY
- Uses a simple character encoder
- Only for learning purpose, and get more insight into the gpt models
- Layer norms and residuel connections are done differently in gpt
- Using masked attention heads for each, should not be needed?

## When running on a old laptop

After some training it can output (1000 iterations)

the profound barbarbs; though, the body had entribulated contetion ager
still snarphed, but there, becleks or the augled two Newshipe, but the
best a toll-rail bear-nower of the latternal hundred by two whalemen,
drawing which never former can it is known, yies futens, and cannibal
an would seement a boats.

After some more training (1500 iterations)

Night which the nature like my float truning but the truth last, but worse
little yeards when glass they faying noiled the bloody, and Queequeg his
tedy, and side it abanneathen; and from the look is he didnâ€™t makes; in
the monster the case of _himself man whale_; for these though is up for
one vial.

After some more training (5000 iterations)

All the same he calls the waves, Tit-bred box had accomple
provinced to Charleys, though terrors shoot in the hump handle the ship
eternal was done, and the waves the appearance rock, more thin so
downward-evolent, and had scruting up their line; the same of absorbing all
themselves, the transfixted half-speed, when to capt on this tropic,
intent pospect the deck the banket-pocket itself with a dark. Striking,
he pot his eyes, and ascalsed him the handspike with that mad story

## Summary
Quite nonsense data, bigger dataset, bigger model, smarter optimizations and better hardware will most likely produce better results
